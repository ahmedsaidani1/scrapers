services:
  - type: cron
    name: powerbi-sanundo
    env: python
    schedule: "0 2 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: sanundo
      - key: POWERBI_WORKSHEET_NAME
        value: sanundo
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-heima24
    env: python
    schedule: "20 2 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: heima24
      - key: POWERBI_WORKSHEET_NAME
        value: heima24
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-st-shop24
    env: python
    schedule: "40 2 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: st_shop24
      - key: POWERBI_WORKSHEET_NAME
        value: st_shop24
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-selfio
    env: python
    schedule: "0 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: selfio
      - key: POWERBI_WORKSHEET_NAME
        value: selfio
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-heizungsdiscount24
    env: python
    schedule: "20 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: heizungsdiscount24
      - key: POWERBI_WORKSHEET_NAME
        value: heizungsdiscount24
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "5000"

  - type: cron
    name: powerbi-meinhausshop-p1
    env: python
    schedule: "40 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: MEINHAUSSHOP_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: meinhausshop
      - key: MEINHAUSSHOP_SITEMAP_PARTS
        value: "1"
      - key: MEINHAUSSHOP_SITEMAP_TIMEOUT
        value: "45"
      - key: MEINHAUSSHOP_SITEMAP_RETRIES
        value: "3"
      - key: POWERBI_WORKSHEET_NAME
        value: meinhausshop_p1
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "7000"
      - key: SCRAPER_WORKERS
        value: "14"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "80"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "80"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "12"
      - key: SCRAPER_MAX_RETRIES
        value: "1"
      - key: SCRAPER_RETRY_DELAY
        value: "0.2"
      - key: SCRAPER_MIN_DELAY
        value: "0.00"
      - key: SCRAPER_MAX_DELAY
        value: "0.01"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "1000"
      - key: SHEETS_BATCH_SIZE
        value: "5000"

  - type: cron
    name: powerbi-meinhausshop-p2
    env: python
    schedule: "45 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: MEINHAUSSHOP_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: meinhausshop
      - key: MEINHAUSSHOP_SITEMAP_PARTS
        value: "2"
      - key: MEINHAUSSHOP_SITEMAP_TIMEOUT
        value: "45"
      - key: MEINHAUSSHOP_SITEMAP_RETRIES
        value: "3"
      - key: POWERBI_WORKSHEET_NAME
        value: meinhausshop_p2
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "7000"
      - key: SCRAPER_WORKERS
        value: "14"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "80"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "80"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "12"
      - key: SCRAPER_MAX_RETRIES
        value: "1"
      - key: SCRAPER_RETRY_DELAY
        value: "0.2"
      - key: SCRAPER_MIN_DELAY
        value: "0.00"
      - key: SCRAPER_MAX_DELAY
        value: "0.01"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "1000"
      - key: SHEETS_BATCH_SIZE
        value: "5000"

  - type: cron
    name: powerbi-meinhausshop-p3
    env: python
    schedule: "50 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: MEINHAUSSHOP_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: meinhausshop
      - key: MEINHAUSSHOP_SITEMAP_PARTS
        value: "3"
      - key: MEINHAUSSHOP_SITEMAP_TIMEOUT
        value: "45"
      - key: MEINHAUSSHOP_SITEMAP_RETRIES
        value: "3"
      - key: POWERBI_WORKSHEET_NAME
        value: meinhausshop_p3
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "7000"
      - key: SCRAPER_WORKERS
        value: "14"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "80"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "80"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "12"
      - key: SCRAPER_MAX_RETRIES
        value: "1"
      - key: SCRAPER_RETRY_DELAY
        value: "0.2"
      - key: SCRAPER_MIN_DELAY
        value: "0.00"
      - key: SCRAPER_MAX_DELAY
        value: "0.01"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "1000"
      - key: SHEETS_BATCH_SIZE
        value: "5000"

  - type: cron
    name: powerbi-meinhausshop-p4
    env: python
    schedule: "55 3 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: MEINHAUSSHOP_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: meinhausshop
      - key: MEINHAUSSHOP_SITEMAP_PARTS
        value: "4"
      - key: MEINHAUSSHOP_SITEMAP_TIMEOUT
        value: "45"
      - key: MEINHAUSSHOP_SITEMAP_RETRIES
        value: "3"
      - key: POWERBI_WORKSHEET_NAME
        value: meinhausshop_p4
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "7000"
      - key: SCRAPER_WORKERS
        value: "14"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "80"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "80"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "12"
      - key: SCRAPER_MAX_RETRIES
        value: "1"
      - key: SCRAPER_RETRY_DELAY
        value: "0.2"
      - key: SCRAPER_MIN_DELAY
        value: "0.00"
      - key: SCRAPER_MAX_DELAY
        value: "0.01"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "1000"
      - key: SHEETS_BATCH_SIZE
        value: "5000"

  - type: cron
    name: powerbi-wolfonlineshop
    env: python
    schedule: "20 4 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: SCRAPER_FILTER
        value: wolfonlineshop
      - key: POWERBI_WORKSHEET_NAME
        value: wolfonlineshop
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "5"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-pumpe24
    env: python
    schedule: "40 4 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: PUMPE24_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: pumpe24
      - key: POWERBI_WORKSHEET_NAME
        value: pumpe24
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "5"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-pumpenheizung
    env: python
    schedule: "0 5 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: PUMPENHEIZUNG_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: pumpenheizung
      - key: POWERBI_WORKSHEET_NAME
        value: pumpenheizung
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"

  - type: cron
    name: powerbi-wasserpumpe
    env: python
    schedule: "20 5 * * 0"
    buildCommand: pip install -r requirements.txt
    startCommand: python run_production_powerbi.py
    envVars:
      - key: GOOGLE_APPLICATION_CREDENTIALS_JSON
        sync: false
      - key: WASSERPUMPE_PROXY
        sync: false
      - key: SCRAPER_FILTER
        value: wasserpumpe
      - key: POWERBI_WORKSHEET_NAME
        value: wasserpumpe
      - key: PIPELINE_BATCH_SIZE
        value: "1"
      - key: PIPELINE_MEMORY_LIMIT_MB
        value: "1700"
      - key: SCRAPER_WORKERS
        value: "6"
      - key: SCRAPER_HTTP_POOL_CONNECTIONS
        value: "30"
      - key: SCRAPER_HTTP_POOL_MAXSIZE
        value: "30"
      - key: SCRAPER_REQUEST_TIMEOUT
        value: "20"
      - key: SCRAPER_MAX_RETRIES
        value: "2"
      - key: SCRAPER_RETRY_DELAY
        value: "1"
      - key: SCRAPER_MIN_DELAY
        value: "0.01"
      - key: SCRAPER_MAX_DELAY
        value: "0.08"
      - key: SCRAPER_CSV_BUFFER_SIZE
        value: "300"
      - key: SHEETS_BATCH_SIZE
        value: "1200"
